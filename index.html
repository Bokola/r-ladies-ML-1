<!DOCTYPE html>
<html>
  <head>
    <title>Machine Learning 101</title>
    <meta charset="utf-8">
    <meta name="author" content="  Sarah Romanes   <span>&lt;i class="fab  fa-twitter faa-float animated "&gt;&lt;/i&gt;&amp;nbsp;@sarahromanes</span>" />
    <link href="libs/remark-css/kunoichi.css" rel="stylesheet" />
    <link href="libs/remark-css/ninjutsu.css" rel="stylesheet" />
    <link href="libs/font-awesome-animation/font-awesome-animation-emi.css" rel="stylesheet" />
    <script src="libs/fontawesome/js/fontawesome-all.min.js"></script>
    <link href="libs/font-awesome/css/fontawesome-all.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="assets\custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Machine Learning 101
## Supervised Learning in R
### <br><br>Sarah Romanes   <span>&lt;i class="fab  fa-twitter faa-float animated "&gt;&lt;/i&gt;&amp;nbsp;@sarahromanes</span>
### <br>2018/10/10<br><br><span>&lt;i class="fas  fa-link faa-vertical animated " style=" color:white;"&gt;&lt;/i&gt;&amp;nbsp;bit.ly/rladies-sydney-ML-1</span>

---





layout: false
class: bg-main3 split-30 hide-slide-number

.column[

]
.column.slide-in-right[.content.vmiddle[
.sliderbox.shade_main.pad1[
.font5[Welcome]
]
]]

---

class: split-two white

.column.bg-main1[.content.vmiddle.center[

# Overview

&lt;br&gt;

### This two part R-Ladies workshop is designed to give you a small taster into the large field that is known as .purple[**Machine Learning**]! Today, we will cover supervised learning techniques (explained later), and next week we will cover model performance assessment.

&lt;br&gt;

### For a more indepth explanation of topics covered, please read the *free* Introduction to Statistical Learning textbook (James, Witten, Hastie, and Tibshirani) [here](http://www-bcf.usc.edu/~gareth/ISL/).

]]
.column.bg-main3[.content.vmiddle.center[
&lt;center&gt;
 &lt;img src="images/ISLR.jpg", width="70%"&gt;

]]



---

class: middle center bg-main1

&lt;img src="images/mlmeme.jpg", width="70%"&gt;


---

# .purple[What *is* Machine Learning?]

&lt;br&gt;

### Machine learning is concerned with finding functions that best **predict** outputs (responses), given data inputs (predictors).

&lt;br&gt;

&lt;center&gt;

  &lt;img src="images/ml-process.png", width="60%"&gt;

&lt;/center&gt;

&lt;br&gt;

### Mathematically, Machine Learning problems are simply *optimisation* problems, in which we will use <i class="fab  fa-r-project "></i> to help us solve!

---

# .purple[Why do Machine Learning in <i class="fab  fa-r-project "></i>?]

&lt;br&gt;

&lt;center&gt;

  &lt;img src="images/python-r-other-2016-2017.jpg", width="70%"&gt;

&lt;/center&gt;

---

# .purple[Supervised vs Unsupervised Learning]


---

layout: false
class: bg-main3 split-30 hide-slide-number

.column[

]
.column.slide-in-right[.content.vmiddle[
.sliderbox.shade_main.pad1[
.font5[Regression]
]
]]

---

class: split-two white 

.column.bg-main1[.content[
&lt;br&gt;

# A simple prediction problem

&lt;br&gt;

### Consider the simulated dataset `Income`, which looks at the relationship between `Education` (years) and `Income` (thousands).


```r
data &lt;- read.csv("data/Income.csv")
head(data)
```

```
##   Education   Income
## 1  10.00000 26.65884
## 2  10.40134 27.30644
## 3  10.84281 22.13241
## 4  11.24415 21.16984
## 5  11.64548 15.19263
## 6  12.08696 26.39895
```

### What shape is the relationship? Can we build a function to predict the value of a new data point?

]]

.column[.content.vmiddle.center[


&lt;img src="index_files/figure-html/unnamed-chunk-2-1.png" width="504" /&gt;


]]


---

class: middle center bg-main1

## Linear Regression to the Rescue!

&lt;img src="images/ols.png", width="70%"&gt;

---

# Shiny app

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in <i class="fab  fa-r-project "></i> using the `lm` function as follows:

&lt;br&gt;





```r
*fit &lt;-  lm(data=data,
            Income ~ Education)
```
]]
.column.bg-main3[.content.vmiddle.center[
# This tells the `lm` function what data we are referring to.
]]

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in <i class="fab  fa-r-project "></i> using the `lm` function as follows:

&lt;br&gt;


```r
fit &lt;-  lm(data=data, 
*           Income ~ Education)
```
]]
.column.bg-main3[.content.vmiddle.center[
## This tells the `lm` function what variables we would like to regress. 

### R expects the relationship in the form of `response~predictors`. 
]]

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in <i class="fab  fa-r-project "></i> using the `lm` function as follows:



```r
fit &lt;-  lm(data=data, 
            Income ~ Education)
*summary(fit)
```


```r
Call:
lm(formula = Income ~ Education, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-13.046  -2.293   0.472   3.288  10.110 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
*(Intercept) -39.4463     4.7248  -8.349  4.4e-09 ***
Education     5.5995     0.2882  19.431  &lt; 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.653 on 28 degrees of freedom
Multiple R-squared:  0.931,	Adjusted R-squared:  0.9285 
F-statistic: 377.6 on 1 and 28 DF,  p-value: &lt; 2.2e-16
```



]]
.column.bg-main3[.content.vmiddle.center[

## We can use `summary` function to examine regression coefficients, and information about the residuals of our model.
]]

---


class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in <i class="fab  fa-r-project "></i> using the `lm` function as follows:



```r
fit &lt;-  lm(data=data, 
            Income ~ Education)
*summary(fit)
```


```r
Call:
lm(formula = Income ~ Education, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-13.046  -2.293   0.472   3.288  10.110 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -39.4463     4.7248  -8.349  4.4e-09 *** 
*Education     5.5995     0.2882  19.431  &lt; 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.653 on 28 degrees of freedom
Multiple R-squared:  0.931,	Adjusted R-squared:  0.9285 
F-statistic: 377.6 on 1 and 28 DF,  p-value: &lt; 2.2e-16
```



]]
.column.bg-main3[.content.vmiddle.center[

## We can use `summary` function to examine regression coefficients, and information about the residuals of our model.
]]

---


class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in <i class="fab  fa-r-project "></i> using the `lm` function as follows:



```r
fit &lt;-  lm(data=data, 
            Income ~ Education)

New_Data &lt;-  data.frame(Education = c(15, 18))
*predict(fit, New_Data)
```

```
##        1        2 
## 44.54599 61.34444
```

]]
.column.bg-main3[.content.vmiddle.center[

## Using the `predict` function, we can predict the `Income` of a new `Education` value (or values). 
]]

---

class: split-two white 

.column.bg-main1[.content[

# Modelling Binary Outcomes

&lt;br&gt;

### Consider the dataset `Spiders`, Suzuki et al. (2006) , from a study which looked at the relationship between `GrainSize` of sand and `Spiders` presence.


```r
data &lt;- read.csv("data/Spiders.csv")
head(data)
```

```
##   GrainSize Spiders
## 1     0.245       0
## 2     0.247       0
## 3     0.285       1
## 4     0.299       1
## 5     0.327       1
## 6     0.347       1
```

### Can we use `lm` to predict the class for a new data point?

]]

.column[.content.vmiddle.center[


&lt;img src="index_files/figure-html/unnamed-chunk-12-1.png" width="504" /&gt;


]]



---


class: split-two white

.column.bg-main1[.content[

## To model **binary data**, we need to **link** our **predictors** to our response using a *link function*.

&lt;br&gt;

### Instead of predicting the outcome directly, we instead predict the probability of being class 1, given the linear combination of predictors, as follows:

$$ p(y=1|\\beta_0 + \\beta_1 x)  = \\sigma(\\beta_0 + \\beta_1 x) $$

For the logistic (`logit`) link

$$ p(y=1|\\beta_0 + \\beta_1 x)  = \\frac{1}{1+ \\exp( - (\\beta_0 + \\beta_1 x))} $$

For the probit (`probit`) link

$$ p(y=1|\\beta_0 + \\beta_1 x)  = \\Phi(\\beta_0 + \\beta_1 x) $$


]]
.column.white[.content.vmiddle.center[
&lt;img src="index_files/figure-html/unnamed-chunk-13-1.png" width="504" /&gt;

]]

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a glm in <i class="fab  fa-r-project "></i> using the `glm` function as follows:

&lt;br&gt;



```r
*fit &lt;-  glm(data=data,
            Spiders~GrainSize,
            family=binomial(link="logit")) 
```
]]
.column.bg-main3[.content.vmiddle.center[
# This tells the `glm` function what data we are referring to.
]]

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a glm in <i class="fab  fa-r-project "></i> using the `glm` function as follows:

&lt;br&gt;


```r
fit &lt;-  glm(data=data, 
*           Spiders~GrainSize,
            family=binomial(link="logit")) 
```
]]
.column.bg-main3[.content.vmiddle.center[
## This tells the `glm` function what variables we would like to regress. Just like the `lm` function, R expects the relationship in the form of `response~predictors`. 
]]

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a glm in <i class="fab  fa-r-project "></i> using the `glm` function as follows:

&lt;br&gt;


```r
fit &lt;-  glm(data=data, 
            Spiders~GrainSize, 
*           family=binomial(link="logit"))
```

&lt;center&gt;
  OR
&lt;/center&gt;


```r
fit &lt;-  glm(data=data, 
            Spiders~GrainSize, 
*           family=binomial(link="probit"))
```


&lt;center&gt;
  OR
&lt;/center&gt;


```r
fit &lt;-  glm(data=data, 
            Spiders~GrainSize, 
*           family=binomial(link="cloglog"))
```

&lt;center&gt;
  and more...
&lt;/center&gt;

]]
.column.bg-main3[.content.vmiddle.center[
## This tells the `glm` function how we would like to model our response. For **binary** response data, we use the `binomial` family. 

## Further, there are many ways we can link our linear combination of predictors to the 0,1 space. 
]]

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a glm in <i class="fab  fa-r-project "></i> using the `glm` function as follows:

&lt;br&gt;


```r
fit &lt;-  glm(data=data, 
            Spiders~GrainSize, 
            family=binomial(link="logit"))
summary(fit)
```

```r
Call:
glm(formula = Spiders ~ GrainSize, family = binomial(link = "logit"), 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7406  -1.0781   0.4837   0.9809   1.2582  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
*(Intercept)   -1.648      1.354  -1.217   0.2237
*GrainSize      5.122      3.006   1.704   0.0884 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
]]
.column.bg-main3[.content.vmiddle.center[
## Similar to the `lm` function, we can use the `summary` function to examine regression coefficients.
]]


---

class: split-60 white

.column.bg-main1[.content[
# We can fit a glm in <i class="fab  fa-r-project "></i> using the `glm` function as follows:

&lt;br&gt;




```r
fit &lt;-  glm(data=data, 
            Spiders~GrainSize, 
            family=binomial(link="logit"))

New_Data &lt;-  data.frame(GrainSize = c(0.4, 0.87))
*probs &lt;- predict(fit, New_Data,type="response")
probs 
```

```
##         1         2 
## 0.5989270 0.9431134
```

```r
*round(probs)
```

```
## 1 2 
## 1 1
```
]]
.column.bg-main3[.content.vmiddle.center[
## And we can also use the `predict` function to estimate class membership probability, as well as use the `round` command to estimate class.
]]


---

class: middle center bg-main1

## Of course, probabilities close to 0.5 are hard to classify!!

&lt;img src="images/response.jpg", width="70%"&gt;

---

class: middle center bg-main1

<span>&lt;i class="fas  fa-exclamation-triangle fa-7x faa-flash animated "&gt;&lt;/i&gt;</span>

# A warning for using GLMs!

---

class: split-two white

.column[.content[


```r
data.new &lt;- read.csv("data/SpidersWarning.csv")
ggplot(data.new, 
       aes(x=GrainSize, y=Spiders)) +
       geom_point(col="red", size=3)
```

&lt;img src="index_files/figure-html/unnamed-chunk-23-1.png" width="504" /&gt;
]]

.column.bg-main3[.content.vmiddle.center[
## Suppose the scientist who collected their data thought the study would look more convincing if the data was **perfectly** sepatated, and sneakily modified their results. How would their glm look?
]]

---

class: split-60 white

.column[.content[


```r
fit &lt;-  glm(data=data.new, 
            Spiders~GrainSize, 
            family=binomial(link="logit"))
```

```
## Warning: glm.fit: algorithm did not converge
```

```
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
```


]]

.column.bg-main3[.content.vmiddle.center[
## Warnings appear when we try and fit this glm...
]]

---

class: split-60 white

.column[.content[


```r
fit &lt;-  glm(data=data.new, 
            Spiders~GrainSize, 
            family=binomial(link="logit"))
```

```
## Warning: glm.fit: algorithm did not converge
```

```
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
```

```r
*summary(fit)
```

```r
Call:
glm(formula = Spiders ~ GrainSize, family = binomial(link = "logit"), 
    data = data.new)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.087e-05  -2.100e-08  -2.100e-08   2.100e-08   7.488e-05  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
*(Intercept)   -912.4   362618.2  -0.003    0.998
*GrainSize     1569.2   624478.6   0.003    0.998
```

]]

.column.bg-main3[.content.vmiddle.center[
## Warnings appear when we try and fit this glm...

## ...and, when we look at the `summary`, we can see our regression coefficients are blowing up, as well as the standard errors associated with them! 
]]


---

class: middle center bg-main1

&lt;img src="images/warning.jpg", width="70%"&gt;

---

class: split-two white 

.column.bg-main1[.content[

&lt;br&gt;

# Multiple Regression

&lt;br&gt;

### Can we fit a model with more than one predictor? Of course we can! Consider the dataset `Exam`, where two exam scores are given for each student, and a Label represents whether they passed or failed the course.


```r
data&lt;- read.csv("data/Exam.csv", header=T)
head(data)
```

```
##      Exam1    Exam2 Label
## 1 34.62366 78.02469     0
## 2 30.28671 43.89500     0
## 3 35.84741 72.90220     0
## 4 60.18260 86.30855     1
## 5 79.03274 75.34438     1
## 6 45.08328 56.31637     0
```

]]

.column[.content.vmiddle.center[


&lt;img src="index_files/figure-html/unnamed-chunk-29-1.png" width="504" /&gt;


]]


---

class: split-60 white

.column.bg-main1[.content[
# We can fit the glm in <i class="fab  fa-r-project "></i> using the `glm` function as follows:

&lt;br&gt;


```r
fit &lt;-  glm(data=data, 
*           Label ~ .,
            family=binomial(link="logit")) 
summary(fit)
```

```r
Call:
glm(formula = Label ~ ., family = binomial(link = "logit"), 
    data = data)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.19287  -0.18009   0.01577   0.19578   1.78527  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -25.16133    5.79836  -4.339 1.43e-05 ***
Exam1         0.20623    0.04800   4.297 1.73e-05 ***
Exam2         0.20147    0.04862   4.144 3.42e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
]]
.column.bg-main3[.content.vmiddle.center[
### The formula `Label ~ . ` tells the `glm` funcion to regress against all (two) predictors, `Exam1` and `Exam2`.
]]

---

class: split-two white 

.column.bg-main1[.content[

&lt;br&gt;

# The Decision Boundary

&lt;br&gt;

We can plot the **decision boundary** on our scatterplot for `Exam1` vs `Exam2` by looking at the set of points for which our classifier predicts `$$p(y=1|\beta_0 + \beta_1 \text{Exam1} + \beta_2 \text{Exam2}) \geq 0.5$$`


Given the `logit` link, this is true when 
`$$\beta_0 + \beta_1 \text{Exam1} + \beta_2 \text{Exam2} \geq 0$$`

This is the **decision boundary** and can be rearranged as 
$$ \text{Exam2} \geq \frac{-\beta_0}{\beta_2} + \frac{-\beta_1}{\beta_2} \text{Exam 1} $$

]]

.column[.content.vmiddle.center[


&lt;img src="index_files/figure-html/unnamed-chunk-32-1.png" width="504" /&gt;


]]


---


layout: false
class: bg-main3 split-30 hide-slide-number

.column[

]
.column.slide-in-right[.content.vmiddle[
.sliderbox.shade_main.pad1[
.font5[Classification]
]
]]

---

class: middle center bg-main1

&lt;img src="images/food.jpg", width="65%"&gt;

---


class: split-two white 

.column.bg-main1[.content[

&lt;br&gt;

# A case for K-Nearest Neighbours

&lt;br&gt;

## Consider the dataset `Microchips`,


```r
data &lt;- read.csv("data/Microchips.csv")
head(data)
```

```
##       Test1   Test2 Label
## 1  0.051267 0.69956     1
## 2 -0.092742 0.68494     1
## 3 -0.213710 0.69225     1
## 4 -0.375000 0.50219     1
## 5 -0.513250 0.46564     1
## 6 -0.524770 0.20980     1
```


### Can we use `glm` to predict the class for a new data point?
]]

.column[.content.vmiddle.center[


&lt;img src="index_files/figure-html/unnamed-chunk-34-1.png" width="504" /&gt;


]]

---

# Different data types require different machine learning methods

While we can indeed use Logistic regression to **classify** data points, this simply isn't feasible when

We have high class seperation in our data
We have a non-linear combination of predictors influcing our response


So, what other options do we have?

---
class: split-two white 

.column.bg-main1[.content[

# The K-Nearest Neighbours Algorithm

Credit to Natasha Latysheva for image

]]

.column[.content.vmiddle.center[

&lt;img src="images/knn2-Natasha-Latysheva.jpg", width="80%"&gt;


]]


---

class: split-two white

.column.bg-main1[.content[

&lt;br&gt;

# KNN in <i class="fab  fa-r-project "></i>

&lt;br&gt;


```r
X &lt;- data[,1:2]
cl &lt;- as.factor(data[,3])
```
]]

.column.bg-main3[.content.vmiddle.center[
## First, we split our data into **predictors** (`X`), and response (`cl` - for *class*),

]]


---

class: split-two white pink-code

.column.bg-main1[.content[

&lt;br&gt;

# KNN in <i class="fab  fa-r-project "></i>

&lt;br&gt;


```r
X &lt;- data[,1:2]
cl &lt;- as.factor(data[,3])

library(MASS)
*new.X &lt;- expand.grid(x=seq(min(X[,1]-0.5), max(X[,1]+0.5),
*                          by=0.1),
*                    y=seq(min(X[,2]-0.5), max(X[,2]+0.5),
*                          by=0.1))
```
]]

.column[.content.vmiddle.center[

.black[Next, we need to give the function new data points, as the `knn` function fits and predicts at the same time. We will use the `MASS` package to generate a grid of new points to predict the class of.]

&lt;img src="index_files/figure-html/unnamed-chunk-37-1.png" width="504" /&gt;

]]


---

class: split-two white 

.column.bg-main1[.content[

&lt;br&gt;

# KNN in <i class="fab  fa-r-project "></i>

&lt;br&gt;


```r
X &lt;- data[,1:2]
cl &lt;- as.factor(data[,3])

library(MASS)
new.X &lt;- expand.grid(x=seq(min(X[,1]-0.5), max(X[,1]+0.5),
                           by=0.1),
                     y=seq(min(X[,2]-0.5), max(X[,2]+0.5), 
                           by=0.1))
 
library(class)
*classif &lt;- knn(X, new.X, cl, k = 4, prob=TRUE)
```
]]

.column.bg-main3[.content.vmiddle.center[

## Now we call the `knn` function, putting in our datapoints, our new data, and the classes of the original data points, as well as how many neighbours we want to consider. In this example, we consider `k=4`.
]]


---

class: split-two white 

.column.bg-main1[.content[

&lt;br&gt;

# KNN in <i class="fab  fa-r-project "></i>

&lt;br&gt;


```r
X &lt;- data[,1:2]
cl &lt;- as.factor(data[,3])

library(MASS)
new.X &lt;- expand.grid(x=seq(min(X[,1]-0.5), max(X[,1]+0.5),
                           by=0.1),
                     y=seq(min(X[,2]-0.5), max(X[,2]+0.5), 
                           by=0.1))
 
library(class)
classif &lt;- knn(X, new.X, cl, k = 4, prob=TRUE)
```


&lt;br&gt;

&lt;center&gt;

We can plot the decision boundaries, as well as the classification of our new points, as follows. Detailed R code is in slide raw files.

&lt;/center&gt;

]]

.column[.content.vmiddle.center[

&lt;img src="index_files/figure-html/unnamed-chunk-40-1.png" width="504" /&gt;
]]



---


class: split-two white 

.column.bg-main1[.content[

&lt;br&gt;

# KNN in <i class="fab  fa-r-project "></i>

&lt;br&gt;


```r
X &lt;- data[,1:2]
cl &lt;- as.factor(data[,3])

library(MASS)
new.X &lt;- expand.grid(x=seq(min(X[,1]-0.5), max(X[,1]+0.5),
                           by=0.1),
                     y=seq(min(X[,2]-0.5), max(X[,2]+0.5), 
                           by=0.1))
 
library(class)
*classif &lt;- knn(X, new.X, cl, k = 10, prob=TRUE)
```


&lt;br&gt;

&lt;center&gt;

We can also see how our decisions change based on the number of neighbours we consider! Here, we consider `k=10`.

&lt;/center&gt;

]]

.column[.content.vmiddle.center[

&lt;img src="index_files/figure-html/unnamed-chunk-42-1.png" width="504" /&gt;
]]


---

class: middle center bg-main1

## What other classifiers exist besides KNN?

&lt;img src="images/knn.jpg", width="60%"&gt;



---

class: split-two white

.column[.content[

&lt;br&gt;

# .purple[Decision Trees]

&lt;br&gt;


&lt;img src="images/tree.jpg", width="90%"&gt;

&lt;br&gt;


]]

.column.bg-main1[.content.vmiddle.center[

### A Decision Tree determines the predictive value based on series of questions and conditions.

&lt;br&gt;

###  When making Decision Trees, there are several factors we take into consideration. These include - on what features do we make our decisions on? What is the threshold for classifying each question into a .yellow[**yes**] or .yellow[**no**] answer?

&lt;br&gt;

Image Source: [Egor Dezhic](https://becominghuman.ai)


]]


---

class: pink-code

# .purple[Decision Trees in <i class="fab  fa-r-project "></i> using `rpart`]

## Consider the `iris` dataset


```r
head(iris)
```

```
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
```

## How can we construct a decision tree to determine the `Species` of `iris`?

---

class: split-two white

.column.bg-main1[.content[

&lt;br&gt;

# Trees in <i class="fab  fa-r-project "></i>


```r
library(rpart)
*tree &lt;- rpart(Species ~ ., data = iris, method = "class")
```
]]

.column.bg-main3[.content.vmiddle.center[
## Using the `rpart` function, we build a classification tree, using the formula `Species~.` to indicate that we wish to use all the variables to predict our class.
]]

---

class: split-two white

.column.bg-main1[.content[

&lt;br&gt;

# Trees in <i class="fab  fa-r-project "></i>


```r
library(rpart)
tree &lt;- rpart(Species ~ ., data = iris, method = "class") 

*library(rpart.plot)
*rpart.plot(tree)
```

&lt;br&gt;

### Futher, using the `rpart.plot` function from the `rpart.plot` library, we can visualise the results of our classification tree. 

### Each node shows the predicted class, the predicted probability of each class, and the percentage of observations in each node.

]]

.column[.content.vmiddle.center[

&lt;img src="index_files/figure-html/unnamed-chunk-46-1.png" width="504" height="70%" /&gt;


]]

---

class: split-two white

.column.bg-main1[.content[

&lt;br&gt;

# Trees in <i class="fab  fa-r-project "></i>


```r
library(rpart)
tree &lt;- rpart(Species ~ ., data = iris, method = "class") 

library(rpart.plot) 
rpart.plot(tree)
```

```r
New_Data &lt;-data.frame(Sepal.Length= 5.0, Sepal.Width = 3.9,
                     Petal.Length = 1.4, Petal.Width = 0.3)
*predict(tree, New_Data)
```

```
##   setosa versicolor virginica
## 1      1          0         0
```

&lt;br&gt;

### Using `predict`, we can predict the class of a new data point, much like in the previous examples. Alternatively - we could have predicted using the tree ourselves!  

]]

.column[.content.vmiddle.center[

&lt;img src="index_files/figure-html/unnamed-chunk-49-1.png" width="504" height="70%" /&gt;


]]




---

# A single tree is prone to overfitting

&lt;br&gt;

&lt;center&gt;

  &lt;img src="images/overfit.jpg", width="70%"&gt;

&lt;/center&gt;

&lt;br&gt;


---

class: middle center

# .purple[Ensemble Learning: Bagging]

&lt;img src="images/bagging.png", width="90%"&gt;


---

# Random Forest


---

class: split-two white

.column.bg-main1[.content[

&lt;br&gt;

# `randomForest` in <i class="fab  fa-r-project "></i>


```r
library(randomForest)
*iris.rf &lt;- randomForest(Species ~ ., iris)
iris.rf
```

```
## 
## Call:
##  randomForest(formula = Species ~ ., data = iris) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 4%
## Confusion matrix:
##            setosa versicolor virginica class.error
## setosa         50          0         0        0.00
## versicolor      0         47         3        0.06
## virginica       0          3        47        0.06
```
]]

.column.bg-main3[.content.vmiddle.center[
## Using the `randomForest`  function, we can train our ensemble learning using the same formula we passed to `rpart`.
]]

---

class: split-two white

.column.bg-main1[.content[

&lt;br&gt;

# `randomForest` in <i class="fab  fa-r-project "></i>


```r
library(randomForest)
iris.rf &lt;- randomForest(Species ~ ., iris) 
*predict(iris.rf, New_Data)
```

```
##      1 
## setosa 
## Levels: setosa versicolor virginica
```
]]

.column.bg-main3[.content.vmiddle.center[
## Again, we can use the same `New_Data` values in the `predict` function as before to reach the same prediction we did with `rpart`.

]]

---


class: pink-code

# .purple[A bigger example]

### Let's look at the `SRBCT` dataset, from the package `multiDA`. This data has a response variable `vy`, which is a `factor` of four different cancer subtypes, and gene expression data from 1586 genes. In total, there are 63 observations.


```r
library(multiDA)
class(SRBCT$vy)
```

```
## [1] "factor"
```

```r
table(SRBCT$vy)
```

```
## 
##  1  2  3  4 
##  8 23 12 20
```

```r
dim(SRBCT$mX)
```

```
## [1]   63 1586
```


---

class: split-two white

.column[.content[


```r
*tree.SRBCT &lt;- rpart(SRBCT$vy ~ SRBCT$mX, method = "class")
rpart.plot(tree.SRBCT)
```

&lt;img src="index_files/figure-html/unnamed-chunk-53-1.png" width="504" /&gt;
]]

.column.bg-main3[.content.vmiddle.center[
## Using the r part function
]]

---

class: split-two white

.column[.content[


```r
*SRBCT.rf &lt;- randomForest(SRBCT$mX, SRBCT$vy)
SRBCT.rf
```

```
## 
## Call:
##  randomForest(x = SRBCT$mX, y = SRBCT$vy) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 39
## 
##         OOB estimate of  error rate: 0%
## Confusion matrix:
##   1  2  3  4 class.error
## 1 8  0  0  0           0
## 2 0 23  0  0           0
## 3 0  0 12  0           0
## 4 0  0  0 20           0
```
]]

.column.bg-main3[.content.vmiddle.center[
## Using the `randomForest`  function
]]


---


class: split-two white

.column[.content[


```r
set.seed(1)
*SRBCT.rf &lt;- randomForest(SRBCT$mX, as.numeric(SRBCT$vy))
SRBCT.rf
```

```r
Call:
 randomForest(x = SRBCT$mX, y = as.numeric(SRBCT$vy)) 
*              Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 528

          Mean of squared residuals: 0.1641464
                    % Var explained: 85.07
```

]]

.column.bg-main3[.content.vmiddle.center[

<span>&lt;i class="fas  fa-exclamation-triangle fa-2x faa-flash animated "&gt;&lt;/i&gt;</span> Warning: when using `randomForest` make sure that your response is a **factor** if you want classification. If `SRBCT$vy` had been *numeric*, `randomForest` would have instead tried to *regress* the predictors to predict a continuous response!

]]
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
