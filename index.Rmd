---
title: "Machine Learning 101"
subtitle: "Supervised Learning in R"
author: "<br><br>Sarah Romanes&nbsp;&nbsp;&nbsp;`r anicon::faa('twitter', animate='float', rtext='&nbsp;@sarahromanes')`"
date: "<br>2018/10/10<br><br>`r anicon::faa('link', animate='vertical', rtext='&nbsp;bit.ly/rladies-sydney-ML-1', color='white')`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["kunoichi", "ninjutsu", "assets/custom.css"]
    seal: true 
    self_contained: false
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(ggplot2)
library(plotly)
library(dplyr)
livedemosign <- function(top, left, deg) {
  htmltools::div("Live Demo!", class="faa-flash animated",
                 style=glue::glue("border:solid; border-color:black; position:absolute; top:{top}%; left:{left}%; font-size:36px; padding:4px; background-color:white; color:black;transform:rotate({deg}deg);")
                 )
}

```

layout: false
class: bg-main3 split-30 hide-slide-number

.column[

]
.column.slide-in-right[.content.vmiddle[
.sliderbox.shade_main.pad1[
.font5[Welcome]
]
]]

---

class: split-two white

.column.bg-main1[.content.vmiddle.center[

# Overview

<br>

### This two part R-Ladies workshop is designed to give you a small taster into the large field that is known as .purple[**Machine Learning**]! Today, we will cover supervised learning techniques (explained later), and next week we will cover model performance assessment.

<br>

### For a more indepth explanation of topics covered, please read the *free* Introduction to Statistical Learning textbook (James, Witten, Hastie, and Tibshirani) [here](http://www-bcf.usc.edu/~gareth/ISL/).

]]
.column.bg-main3[.content.vmiddle.center[
<center>
 <img src="images/ISLR.jpg", width="70%">

]]



---

class: middle center bg-main1

<img src="images/mlmeme.png", width="70%">


---

# .purple[What *is* Machine Learning?]

<br>

### Machine learning is concerned with finding functions that best **predict** outputs (responses), given data inputs (predictors).

<br>

<center>

  <img src="images/ml-process.png", width="60%">

</center>

<br>

### Mathematically, Machine Learning problems are simply *optimisation* problems, in which we will use `r icon::fa("r-project", size=1)` to help us solve!

---

# .purple[Why do Machine Learning in `r icon::fa("r-project", size=1)`?]

<br>

<center>

  <img src="images/python-r-other-2016-2017.jpg", width="70%">

</center>

---

# .purple[Supervised vs Unsupervised Learning]


---

layout: false
class: bg-main3 split-30 hide-slide-number

.column[

]
.column.slide-in-right[.content.vmiddle[
.sliderbox.shade_main.pad1[
.font5[Regression]
]
]]

---

# More Elaboration

---

# Regression meme goes here

---

# Shiny app

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in R using the `lm` function as follows:

<br>

```{r, echo=F}
data <- read.csv("data/Income.csv")
```


```{r, eval=F}
fit <-  lm(data=data, #<<
            Income ~ Education)
```
]]
.column.bg-main3[.content.vmiddle.center[
# This tells the `lm` function what data we are referring to
]]

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in R using the `lm` function as follows:

<br>

```{r, eval=F}
fit <-  lm(data=data, 
            Income ~ Education) #<<
```
]]
.column.bg-main3[.content.vmiddle.center[
# This tells the `glm` function what variables we would like to regress. R expects the relationship in the form of `response~predictors`. 
]]

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in R using the `lm` function as follows:


```{r, eval=F}
fit <-  lm(data=data, 
            Income ~ Education)
summary(fit) #<<
```

```{r, eval=F}
Call:
lm(formula = Income ~ Education, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-13.046  -2.293   0.472   3.288  10.110 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -39.4463     4.7248  -8.349  4.4e-09 *** #<<
Education     5.5995     0.2882  19.431  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.653 on 28 degrees of freedom
Multiple R-squared:  0.931,	Adjusted R-squared:  0.9285 
F-statistic: 377.6 on 1 and 28 DF,  p-value: < 2.2e-16
```



]]
.column.bg-main3[.content.vmiddle.center[

# Using the `summary` function, we ....
]]

---


class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in R using the `lm` function as follows:


```{r, eval=F}
fit <-  lm(data=data, 
            Income ~ Education)
summary(fit) #<<
```

```{r, eval=F}
Call:
lm(formula = Income ~ Education, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-13.046  -2.293   0.472   3.288  10.110 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -39.4463     4.7248  -8.349  4.4e-09 *** 
Education     5.5995     0.2882  19.431  < 2e-16 *** #<<
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.653 on 28 degrees of freedom
Multiple R-squared:  0.931,	Adjusted R-squared:  0.9285 
F-statistic: 377.6 on 1 and 28 DF,  p-value: < 2.2e-16
```



]]
.column.bg-main3[.content.vmiddle.center[

# Using the `summary` function, we ....
]]

---


class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in R using the `lm` function as follows:


```{r, eval=T}
fit <-  lm(data=data, 
            Income ~ Education)

New_Data <-  data.frame(Education = c(15, 18))
predict(fit, New_Data) #<<
```

]]
.column.bg-main3[.content.vmiddle.center[

# Using the `predict` function, we ....
]]

---

# Binary outcomes - a case for Logistic Regression


---


class: split-two white

.column.bg-main1[.content[
# To model **binary data**, we need to **link** our **predictors** to our response using a *link function*.



]]
.column.white[.content.vmiddle.center[
```{r, echo=FALSE, fig.retina=4, warning=F, message=F}
x.vals <- rep(seq(-10,10, by=0.1),2)
fit.1 <- 1/(1+exp(-x.vals))
fit.2 <- pnorm(x.vals)
fit <- c(fit.1,fit.2)
Link <- c(rep("logistic", length(x.vals)),rep("probit", length(x.vals)))

library(latex2exp)

data <- data.frame(x=x.vals, y=fit, Link=Link)
ggplot(data, aes(x=x, y=y, color=Link))+geom_line(size=1.4) + xlab(TeX('$\\beta_0 + \\beta_1 x$')) +  ylab(TeX('$p(y=1|\\beta_0 + \\beta_1 x) = Link(\\beta_0 + \\beta_1 x)$')) + theme(text = element_text(size=20))
```

]]

---

# Shiny app

---

# 2 split, glm and predict

---

class: split-two white

.column.bg-main1[.content[
# We can fit a glm in R using the `glm` function as follows:

<br>

```{r, echo=F}
data <- read.csv("data/Spiders.csv")
```


```{r, eval=F}
fit <-  glm(data=data, #<<
            Spiders~GrainSize,
            family=binomial(link="logit")) 
```
]]
.column.bg-main3[.content.vmiddle.center[
# This tells the `glm` function what data we are referring to
]]

---

class: split-two white

.column.bg-main1[.content[
# We can fit a glm in R using the `glm` function as follows:

<br>

```{r, eval=F}
fit <-  glm(data=data, 
            Spiders~GrainSize, #<<
            family=binomial(link="logit")) 
```
]]
.column.bg-main3[.content.vmiddle.center[
# This tells the `glm` function what variables we would like to regress. Just like the `lm` function, R expects the relationship in the form of `response~predictors`. 
]]

---

class: split-two white

.column.bg-main1[.content[
# We can fit a glm in R using the `glm` function as follows:

<br>

```{r, eval=F}
fit <-  glm(data=data, 
            Spiders~GrainSize, 
            family=binomial(link="logit")) #<<
```
]]
.column.bg-main3[.content.vmiddle.center[
## This tells the `glm` function how we would like to model our response. For **binary** response data, we use the `binomial` family. Further, there are many ways we can link our linear combination of predictors to the 0,1 space. Since we are using the **logistic** link, we use `logit` link. Other common links include the `probit` and `cloglog` links.
]]

---
# GLM meme

---

class: middle center bg-main1

`r anicon::faa('exclamation-triangle', animate='flash', size=7)`

# Some considerations when using Logistic Regression!

---

class: split-two white

.column[.content[

```{r, fig.retina=2}
data.new <- read.csv("data/SpidersWarning.csv")
ggplot(data.new, 
       aes(x=GrainSize, y=Spiders)) +
       geom_point(col="red", size=3)
```
]]

.column.bg-main3[.content.vmiddle.center[
## This tells the `glm` function how we would like to model our response. For **binary** response data, we use the `binomial` family. Further, there are many ways we can link our linear combination of predictors to the 0,1 space. Since we are using the **logistic** link, we use `logit` link. Other common links include the `probit` and `cloglog` links.
]]

---

class: split-60 white

.column[.content[

```{r}
fit <-  glm(data=data.new, 
            Spiders~GrainSize, 
            family=binomial(link="logit"))
```

]]

.column.bg-main3[.content.vmiddle.center[
## Warnings appear when we try and fit this glm, and, when we look at the `summary()`...
]]

---

class: split-60 white

.column[.content[

```{r, eval=T}
fit <-  glm(data=data.new, 
            Spiders~GrainSize, 
            family=binomial(link="logit"))
```
```{r, eval=F}
summary(fit) #<<
```
```{r, eval=FALSE}
Call:
glm(formula = Spiders ~ GrainSize, family = binomial(link = "logit"), 
    data = data.new)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.087e-05  -2.100e-08  -2.100e-08   2.100e-08   7.488e-05  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   -912.4   362618.2  -0.003    0.998 #<<
GrainSize     1569.2   624478.6   0.003    0.998 #<<

```

]]

.column.bg-main3[.content.vmiddle.center[
## Warnings appear when we try and fit this glm, and, when we look at the `summary()`...
]]


---

class: middle center bg-main1

<img src="images/warning.jpg", width="70%">

---

# Multiple regression

---

# Example 

---

# The Lasso

---

class: middle center bg-main1

## We can think of LASSO regression as only having a certain amount of coefficient size to allocate, forcing some to get none!


<iframe width="800" height="450" src="https://www.youtube.com/embed/08drNP-tZbI?rel=0&amp;showinfo=0&amp;start=10" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>


---


layout: false
class: bg-main3 split-30 hide-slide-number

.column[

]
.column.slide-in-right[.content.vmiddle[
.sliderbox.shade_main.pad1[
.font5[Classification]
]
]]

---

class: split-two white 

.column.bg-main1[.content[

<br>

# A case for K-Nearest Neighbours

<br>

## Consider the dataset `Microchips`,

```{r, fig.retina=2}
data <- read.csv("data/Microchips.csv")
head(data)
```


### Can we use `glm` to predict the class for a new data point?
]]

.column[.content.vmiddle.center[


```{r, fig.retina=4, echo=FALSE}
ggplot(data, aes(x=Test1, y=Test2, color=factor(Label)))+geom_point(size=4)
```


]]

---

# Different data types require different machine learning methods

While we can indeed use Logistic regression to **classify** data points, this simply isn't feasible when

We have high class seperation in our data
We have a non-linear combination of predictors influcing our response


So, what other options do we have?


---

class: split-two white

.column.bg-main1[.content[

<br>

# KNN in R

<br>

```{r}
X <- data[,1:2]
cl <- as.factor(data[,3])
```
]]

.column.bg-main3[.content.vmiddle.center[
## First, we split our data into **predictors** (`X`), and response (`cl` - for *class*),

]]


---

class: split-two white pink-code

.column.bg-main1[.content[

<br>

# KNN in R

<br>

```{r, warning=FALSE, message=FALSE}
X <- data[,1:2]
cl <- as.factor(data[,3])

library(MASS)
new.X <- expand.grid(x=seq(min(X[,1]-0.5), max(X[,1]+0.5),#<<
                           by=0.1),#<<
                     y=seq(min(X[,2]-0.5), max(X[,2]+0.5), #<<
                           by=0.1)) #<<
```
]]

.column[.content.vmiddle.center[

.black[Next, we need to give the function new data points, as the `knn` function fits and predicts at the same time. We will use the `MASS` package to generate a grid of new points to predict the class of.]

```{r, fig.retina=4, echo=FALSE, warning=FALSE, message=FALSE}
train <- data[,1:2]
cl <- as.factor(data[,3])

require(MASS)

test <- expand.grid(x=seq(min(train[,1]-0.5), max(train[,1]+0.5),
                           by=0.1),
                     y=seq(min(train[,2]-0.5), max(train[,2]+0.5), 
                           by=0.1))
 
require(class)
classif <- knn(train, test, cl, k = 4, prob=TRUE)
prob <- attr(classif, "prob")
 
 
dataf <- bind_rows(mutate(test,
                           prob=prob,
                           cls=1,
                           prob_cls=ifelse(classif==cls,
                                           1, 0)),
                    mutate(test,
                           prob=prob,
                           cls=0,
                           prob_cls=ifelse(classif==cls,
                                           1, 0)))

ggplot(dataf) +geom_point(aes(x=x, y=y), alpha=0.1)

```

]]


---

class: split-two white 

.column.bg-main1[.content[

<br>

# KNN in R

<br>

```{r}
train <- data[,1:2]
cl <- as.factor(data[,3])

library(MASS)
new.X <- expand.grid(x=seq(min(X[,1]-0.5), max(X[,1]+0.5),
                           by=0.1),
                     y=seq(min(X[,2]-0.5), max(X[,2]+0.5), 
                           by=0.1))
 
library(class)
classif <- knn(X, new.X, cl, k = 4, prob=TRUE) #<<
```
]]

.column.bg-main3[.content.vmiddle.center[

## Now we call the `knn` function, putting in our datapoints, our new data, and the classes of the original data points, as well as how many neighbours we want to consider. In this example, we consider `k=4`.
]]


---

class: split-two white 

.column.bg-main1[.content[

<br>

# KNN in R

<br>

```{r}
train <- data[,1:2]
cl <- as.factor(data[,3])

library(MASS)
new.X <- expand.grid(x=seq(min(X[,1]-0.5), max(X[,1]+0.5),
                           by=0.1),
                     y=seq(min(X[,2]-0.5), max(X[,2]+0.5), 
                           by=0.1))
 
library(class)
classif <- knn(X, new.X, cl, k = 4, prob=TRUE)
```


<br>

<center>

We can plot the decision boundaries, as well as the classification of our new points, as follows. Detailed R code is in slide raw files.

</center>

]]

.column[.content.vmiddle.center[

```{r, fig.retina=4, echo=FALSE, warning=FALSE, message=FALSE}
train <- data[,1:2]
cl <- as.factor(data[,3])

 require(MASS)

 test <- expand.grid(x=seq(min(train[,1]-0.5), max(train[,1]+0.5),
                           by=0.1),
                     y=seq(min(train[,2]-0.5), max(train[,2]+0.5), 
                           by=0.1))
 
  require(class)
 classif <- knn(train, test, cl, k = 4, prob=TRUE)
 prob <- attr(classif, "prob")
 
 
 dataf <- bind_rows(mutate(test,
                           prob=prob,
                           cls=1,
                           prob_cls=ifelse(classif==cls,
                                           1, 0)),
                    mutate(test,
                           prob=prob,
                           cls=0,
                           prob_cls=ifelse(classif==cls,
                                           1, 0)))

 
 ggplot(dataf) +
    geom_point(aes(x=x, y=y, col=cls, size=prob),
               data = mutate(test, cls=classif), alpha=0.5) + 
    scale_size(range=c(0.8, 2)) +
    geom_contour(aes(x=x, y=y, z=prob_cls, group=as.factor(cls), color=as.factor(cls)),
                 bins=2,
                 data=dataf) +
    geom_point(aes(x=x, y=y, col=cls),
               size=5,
               data=data.frame(x=train[,1], y=train[,2], cls=cl)) +
    geom_point(aes(x=x, y=y),
               size=5, shape=1,
               data=data.frame(x=train[,1], y=train[,2], cls=cl))
```
]]



---


class: split-two white 

.column.bg-main1[.content[

<br>

# KNN in R

<br>

```{r}
train <- data[,1:2]
cl <- as.factor(data[,3])

library(MASS)
new.X <- expand.grid(x=seq(min(X[,1]-0.5), max(X[,1]+0.5),
                           by=0.1),
                     y=seq(min(X[,2]-0.5), max(X[,2]+0.5), 
                           by=0.1))
 
library(class)
classif <- knn(X, new.X, cl, k = 10, prob=TRUE) #<<
```


<br>

<center>

We can also see how our decisions change based on the number of neighbours we consider! Here, we consider `k=10`.

</center>

]]

.column[.content.vmiddle.center[

```{r, fig.retina=4, echo=FALSE, warning=FALSE, message=FALSE}
train <- data[,1:2]
cl <- as.factor(data[,3])

 require(MASS)

 test <- expand.grid(x=seq(min(train[,1]-0.5), max(train[,1]+0.5),
                           by=0.1),
                     y=seq(min(train[,2]-0.5), max(train[,2]+0.5), 
                           by=0.1))
 
  require(class)
 classif <- knn(train, test, cl, k = 10, prob=TRUE)
 prob <- attr(classif, "prob")
 
 
 dataf <- bind_rows(mutate(test,
                           prob=prob,
                           cls=1,
                           prob_cls=ifelse(classif==cls,
                                           1, 0)),
                    mutate(test,
                           prob=prob,
                           cls=0,
                           prob_cls=ifelse(classif==cls,
                                           1, 0)))

 
 ggplot(dataf) +
    geom_point(aes(x=x, y=y, col=cls, size=prob),
               data = mutate(test, cls=classif), alpha=0.5) + 
    scale_size(range=c(0.8, 2)) +
    geom_contour(aes(x=x, y=y, z=prob_cls, group=as.factor(cls), color=as.factor(cls)),
                 bins=2,
                 data=dataf) +
    geom_point(aes(x=x, y=y, col=cls),
               size=5,
               data=data.frame(x=train[,1], y=train[,2], cls=cl)) +
    geom_point(aes(x=x, y=y),
               size=5, shape=1,
               data=data.frame(x=train[,1], y=train[,2], cls=cl))
```
]]




---

# Decision Trees

---

# Decision Trees in R using `rpart`

---

class: split-two white

.column[.content[

```{r, warning=F, fig.retina=2}
library(rpart)
library(rpart.plot)

tree <- rpart(Species ~ ., data = iris, method = "class")
rpart.plot(tree)
```
]]

.column.bg-main3[.content.vmiddle.center[
## Using the r part function
]]



---

# A single tree is prone to overfitting

<br>

<center>

  <img src="images/overfit.jpg", width="60%">

</center>

<br>


---

# Concept: Bagging


---

# Random Forest


---

class: split-two white

.column[.content[

```{r, warning=F,message=F}
library(randomForest)
set.seed(1)
iris.rf <- randomForest(Species ~ ., iris)
iris.rf
```
]]

.column.bg-main3[.content.vmiddle.center[
## Using the `randomForest`  function
]]

---

class: pink-code

# .purple[A bigger example]

### Let's look at the `SRBCT` dataset, from the package `multiDA`. This data has a response variable `vy`, which is a `factor` of four different cancer subtypes, and gene expression data from 1586 genes. In total, there are 63 observations.

```{r}
library(multiDA)
class(SRBCT$vy)
table(SRBCT$vy)
dim(SRBCT$mX)
```


---

class: split-two white

.column[.content[

```{r, warning=F, fig.retina=2}
tree.SRBCT <- rpart(SRBCT$vy ~ SRBCT$mX, method = "class") #<<
rpart.plot(tree.SRBCT)
```
]]

.column.bg-main3[.content.vmiddle.center[
## Using the r part function
]]

---

class: split-two white

.column[.content[

```{r, warning=F,message=F}
set.seed(1)
SRBCT.rf <- randomForest(SRBCT$mX, SRBCT$vy) #<<
SRBCT.rf
```
]]

.column.bg-main3[.content.vmiddle.center[
## Using the `randomForest`  function
]]


---


class: split-two white

.column[.content[

```{r, warning=F,message=F, eval=F}
set.seed(1)
SRBCT.rf <- randomForest(SRBCT$mX, as.numeric(SRBCT$vy)) #<<
SRBCT.rf
```
```{r, eval=F}
Call:
 randomForest(x = SRBCT$mX, y = as.numeric(SRBCT$vy)) 
               Type of random forest: regression #<<
                     Number of trees: 500
No. of variables tried at each split: 528

          Mean of squared residuals: 0.1641464
                    % Var explained: 85.07
```

]]

.column.bg-main3[.content.vmiddle.center[

`r anicon::faa('exclamation-triangle', animate='flash', size=2)` Warning: when using `randomForest` make sure that your response is a **factor** if you want classification. If `SRBCT$vy` had been *numeric*, `randomForest` would have instead tried to *regress* the predictors to predict a continuous response!

]] 

