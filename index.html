<!DOCTYPE html>
<html>
  <head>
    <title>Machine Learning 101</title>
    <meta charset="utf-8">
    <meta name="author" content="  Sarah Romanes   <span>&lt;i class="fab  fa-twitter faa-float animated "&gt;&lt;/i&gt;&amp;nbsp;@sarahromanes</span>" />
    <link href="libs/remark-css/kunoichi.css" rel="stylesheet" />
    <link href="libs/remark-css/ninjutsu.css" rel="stylesheet" />
    <link href="libs/font-awesome-animation/font-awesome-animation-emi.css" rel="stylesheet" />
    <script src="libs/fontawesome/js/fontawesome-all.min.js"></script>
    <link href="libs/font-awesome/css/fontawesome-all.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="assets\custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Machine Learning 101
## Supervised Learning in R
### <br><br>Sarah Romanes   <span>&lt;i class="fab  fa-twitter faa-float animated "&gt;&lt;/i&gt;&amp;nbsp;@sarahromanes</span>
### <br>2018/10/10<br><br><span>&lt;i class="fas  fa-link faa-vertical animated " style=" color:white;"&gt;&lt;/i&gt;&amp;nbsp;bit.ly/rladies-sydney-ML-1</span>

---





layout: false
class: bg-main3 split-30 hide-slide-number

.column[

]
.column.slide-in-right[.content.vmiddle[
.sliderbox.shade_main.pad1[
.font5[Welcome]
]
]]

---

class: split-two white

.column.bg-main1[.content.vmiddle.center[

# Overview

&lt;br&gt;

### This two part R-Ladies workshop is designed to give you a small taster into the large field that is known as .purple[**Machine Learning**]! Today, we will cover supervised learning techniques (explained later), and next week we will cover model performance assessment.

&lt;br&gt;

### For a more indepth explanation of topics covered, please read the *free* Introduction to Statistical Learning textbook (James, Witten, Hastie, and Tibshirani) [here](http://www-bcf.usc.edu/~gareth/ISL/).

]]
.column.bg-main3[.content.vmiddle.center[
&lt;center&gt;
 &lt;img src="images/ISLR.jpg", width="70%"&gt;

]]



---

class: middle center bg-main1

&lt;img src="images/mlmeme.png", width="70%"&gt;


---

# .purple[What *is* Machine Learning?]

&lt;br&gt;

### Machine learning is concerned with finding functions that best **predict** outputs (responses), given data inputs (predictors).

&lt;br&gt;

&lt;center&gt;

  &lt;img src="images/ml-process.png", width="60%"&gt;

&lt;/center&gt;

&lt;br&gt;

### Mathematically, Machine Learning problems are simply *optimisation* problems, in which we will use <i class="fab  fa-r-project "></i> to help us solve!

---

# .purple[Why do Machine Learning in <i class="fab  fa-r-project "></i>?]

&lt;br&gt;

&lt;center&gt;

  &lt;img src="images/python-r-other-2016-2017.jpg", width="70%"&gt;

&lt;/center&gt;

---

# .purple[Supervised vs Unsupervised Learning]


---

layout: false
class: bg-main3 split-30 hide-slide-number

.column[

]
.column.slide-in-right[.content.vmiddle[
.sliderbox.shade_main.pad1[
.font5[Regression]
]
]]

---

# More Elaboration

---

# Regression meme goes here

---

# Shiny app

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in R using the `lm` function as follows:

&lt;br&gt;





```r
*fit &lt;-  lm(data=data,
            Income ~ Education)
```
]]
.column.bg-main3[.content.vmiddle.center[
# This tells the `lm` function what data we are referring to
]]

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in R using the `lm` function as follows:

&lt;br&gt;


```r
fit &lt;-  lm(data=data, 
*           Income ~ Education)
```
]]
.column.bg-main3[.content.vmiddle.center[
# This tells the `glm` function what variables we would like to regress. R expects the relationship in the form of `response~predictors`. 
]]

---

class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in R using the `lm` function as follows:



```r
fit &lt;-  lm(data=data, 
            Income ~ Education)
*summary(fit)
```


```r
Call:
lm(formula = Income ~ Education, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-13.046  -2.293   0.472   3.288  10.110 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
*(Intercept) -39.4463     4.7248  -8.349  4.4e-09 ***
Education     5.5995     0.2882  19.431  &lt; 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.653 on 28 degrees of freedom
Multiple R-squared:  0.931,	Adjusted R-squared:  0.9285 
F-statistic: 377.6 on 1 and 28 DF,  p-value: &lt; 2.2e-16
```



]]
.column.bg-main3[.content.vmiddle.center[

# Using the `summary` function, we ....
]]

---


class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in R using the `lm` function as follows:



```r
fit &lt;-  lm(data=data, 
            Income ~ Education)
*summary(fit)
```


```r
Call:
lm(formula = Income ~ Education, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-13.046  -2.293   0.472   3.288  10.110 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -39.4463     4.7248  -8.349  4.4e-09 *** 
*Education     5.5995     0.2882  19.431  &lt; 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.653 on 28 degrees of freedom
Multiple R-squared:  0.931,	Adjusted R-squared:  0.9285 
F-statistic: 377.6 on 1 and 28 DF,  p-value: &lt; 2.2e-16
```



]]
.column.bg-main3[.content.vmiddle.center[

# Using the `summary` function, we ....
]]

---


class: split-60 white

.column.bg-main1[.content[
# We can fit a linear model in R using the `lm` function as follows:



```r
fit &lt;-  lm(data=data, 
            Income ~ Education)

New_Data &lt;-  data.frame(Education = c(15, 18))
*predict(fit, New_Data)
```

```
##        1        2 
## 44.54599 61.34444
```

]]
.column.bg-main3[.content.vmiddle.center[

# Using the `predict` function, we ....
]]

---

# Binary outcomes - a case for Logistic Regression


---


class: split-two white

.column.bg-main1[.content[
# To model **binary data**, we need to **link** our **predictors** to our response using a *link function*.



]]
.column.white[.content.vmiddle.center[
&lt;img src="index_files/figure-html/unnamed-chunk-9-1.png" width="504" /&gt;

]]

---

# Shiny app

---

# 2 split, glm and predict

---

class: split-two white

.column.bg-main1[.content[
# We can fit a glm in R using the `glm` function as follows:

&lt;br&gt;





```r
*fit &lt;-  glm(data=data,
            Spiders~GrainSize,
            family=binomial(link="logit")) 
```
]]
.column.bg-main3[.content.vmiddle.center[
# This tells the `glm` function what data we are referring to
]]

---

class: split-two white

.column.bg-main1[.content[
# We can fit a glm in R using the `glm` function as follows:

&lt;br&gt;


```r
fit &lt;-  glm(data=data, 
*           Spiders~GrainSize,
            family=binomial(link="logit")) 
```
]]
.column.bg-main3[.content.vmiddle.center[
# This tells the `glm` function what variables we would like to regress. Just like the `lm` function, R expects the relationship in the form of `response~predictors`. 
]]

---

class: split-two white

.column.bg-main1[.content[
# We can fit a glm in R using the `glm` function as follows:

&lt;br&gt;


```r
fit &lt;-  glm(data=data, 
            Spiders~GrainSize, 
*           family=binomial(link="logit"))
```
]]
.column.bg-main3[.content.vmiddle.center[
## This tells the `glm` function how we would like to model our response. For **binary** response data, we use the `binomial` family. Further, there are many ways we can link our linear combination of predictors to the 0,1 space. Since we are using the **logistic** link, we use `logit` link. Other common links include the `probit` and `cloglog` links.
]]

---
# GLM meme

---

class: middle center bg-main1

<span>&lt;i class="fas  fa-exclamation-triangle fa-7x faa-flash animated "&gt;&lt;/i&gt;</span>

# Some considerations when using Logistic Regression!

---

class: split-two white

.column[.content[


```r
data.new &lt;- read.csv("data/SpidersWarning.csv")
ggplot(data.new, 
       aes(x=GrainSize, y=Spiders)) +
       geom_point(col="red", size=3)
```

&lt;img src="index_files/figure-html/unnamed-chunk-14-1.png" width="504" /&gt;
]]

.column.bg-main3[.content.vmiddle.center[
## This tells the `glm` function how we would like to model our response. For **binary** response data, we use the `binomial` family. Further, there are many ways we can link our linear combination of predictors to the 0,1 space. Since we are using the **logistic** link, we use `logit` link. Other common links include the `probit` and `cloglog` links.
]]

---

class: split-60 white

.column[.content[


```r
fit &lt;-  glm(data=data.new, 
            Spiders~GrainSize, 
            family=binomial(link="logit"))
```

```
## Warning: glm.fit: algorithm did not converge
```

```
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
```

]]

.column.bg-main3[.content.vmiddle.center[
## Warnings appear when we try and fit this glm, and, when we look at the `summary()`...
]]

---

class: split-60 white

.column[.content[


```r
fit &lt;-  glm(data=data.new, 
            Spiders~GrainSize, 
            family=binomial(link="logit"))
```

```
## Warning: glm.fit: algorithm did not converge
```

```
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
```

```r
*summary(fit)
```

```r
Call:
glm(formula = Spiders ~ GrainSize, family = binomial(link = "logit"), 
    data = data.new)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.087e-05  -2.100e-08  -2.100e-08   2.100e-08   7.488e-05  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
*(Intercept)   -912.4   362618.2  -0.003    0.998
*GrainSize     1569.2   624478.6   0.003    0.998
```

]]

.column.bg-main3[.content.vmiddle.center[
## Warnings appear when we try and fit this glm, and, when we look at the `summary()`...
]]


---

class: middle center bg-main1

&lt;img src="images/warning.jpg", width="70%"&gt;

---

# Multiple regression

---

# Example 

---

# The Lasso

---

class: middle center bg-main1

## We can think of LASSO regression as only having a certain amount of coefficient size to allocate, forcing some to get none!


&lt;iframe width="800" height="450" src="https://www.youtube.com/embed/08drNP-tZbI?rel=0&amp;amp;showinfo=0&amp;amp;start=10" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;


---


layout: false
class: bg-main3 split-30 hide-slide-number

.column[

]
.column.slide-in-right[.content.vmiddle[
.sliderbox.shade_main.pad1[
.font5[Classification]
]
]]

---

class: split-two white 

.column.bg-main1[.content[

&lt;br&gt;

# A case for K-Nearest Neighbours

&lt;br&gt;

## Consider the dataset `Microchips`,


```r
data &lt;- read.csv("data/Microchips.csv")
head(data)
```

```
##       Test1   Test2 Label
## 1  0.051267 0.69956     1
## 2 -0.092742 0.68494     1
## 3 -0.213710 0.69225     1
## 4 -0.375000 0.50219     1
## 5 -0.513250 0.46564     1
## 6 -0.524770 0.20980     1
```


### Can we use `glm` to predict the class for a new data point?
]]

.column[.content.vmiddle.center[


&lt;img src="index_files/figure-html/unnamed-chunk-20-1.png" width="504" /&gt;


]]

---

# Different data types require different machine learning methods

While we can indeed use Logistic regression to **classify** data points, this simply isn't feasible when

We have high class seperation in our data
We have a non-linear combination of predictors influcing our response


So, what other options do we have?


---

class: split-two white

.column.bg-main1[.content[

&lt;br&gt;

# KNN in R

&lt;br&gt;


```r
X &lt;- data[,1:2]
cl &lt;- as.factor(data[,3])
```
]]

.column.bg-main3[.content.vmiddle.center[
## First, we split our data into **predictors** (`X`), and response (`cl` - for *class*),

]]


---

class: split-two white pink-code

.column.bg-main1[.content[

&lt;br&gt;

# KNN in R

&lt;br&gt;


```r
X &lt;- data[,1:2]
cl &lt;- as.factor(data[,3])

library(MASS)
*new.X &lt;- expand.grid(x=seq(min(X[,1]-0.5), max(X[,1]+0.5),
*                          by=0.1),
*                    y=seq(min(X[,2]-0.5), max(X[,2]+0.5),
*                          by=0.1))
```
]]

.column[.content.vmiddle.center[

.black[Next, we need to give the function new data points, as the `knn` function fits and predicts at the same time. We will use the `MASS` package to generate a grid of new points to predict the class of.]

&lt;img src="index_files/figure-html/unnamed-chunk-23-1.png" width="504" /&gt;

]]


---

class: split-two white 

.column.bg-main1[.content[

&lt;br&gt;

# KNN in R

&lt;br&gt;


```r
train &lt;- data[,1:2]
cl &lt;- as.factor(data[,3])

library(MASS)
new.X &lt;- expand.grid(x=seq(min(X[,1]-0.5), max(X[,1]+0.5),
                           by=0.1),
                     y=seq(min(X[,2]-0.5), max(X[,2]+0.5), 
                           by=0.1))
 
library(class)
*classif &lt;- knn(X, new.X, cl, k = 4, prob=TRUE)
```
]]

.column.bg-main3[.content.vmiddle.center[

## Now we call the `knn` function, putting in our datapoints, our new data, and the classes of the original data points, as well as how many neighbours we want to consider. In this example, we consider `k=4`.
]]


---

class: split-two white 

.column.bg-main1[.content[

&lt;br&gt;

# KNN in R

&lt;br&gt;


```r
train &lt;- data[,1:2]
cl &lt;- as.factor(data[,3])

library(MASS)
new.X &lt;- expand.grid(x=seq(min(X[,1]-0.5), max(X[,1]+0.5),
                           by=0.1),
                     y=seq(min(X[,2]-0.5), max(X[,2]+0.5), 
                           by=0.1))
 
library(class)
classif &lt;- knn(X, new.X, cl, k = 4, prob=TRUE)
```


&lt;br&gt;

&lt;center&gt;

We can plot the decision boundaries, as well as the classification of our new points, as follows. Detailed R code is in slide raw files.

&lt;/center&gt;

]]

.column[.content.vmiddle.center[

&lt;img src="index_files/figure-html/unnamed-chunk-26-1.png" width="504" /&gt;
]]



---


class: split-two white 

.column.bg-main1[.content[

&lt;br&gt;

# KNN in R

&lt;br&gt;


```r
train &lt;- data[,1:2]
cl &lt;- as.factor(data[,3])

library(MASS)
new.X &lt;- expand.grid(x=seq(min(X[,1]-0.5), max(X[,1]+0.5),
                           by=0.1),
                     y=seq(min(X[,2]-0.5), max(X[,2]+0.5), 
                           by=0.1))
 
library(class)
*classif &lt;- knn(X, new.X, cl, k = 10, prob=TRUE)
```


&lt;br&gt;

&lt;center&gt;

We can also see how our decisions change based on the number of neighbours we consider! Here, we consider `k=10`.

&lt;/center&gt;

]]

.column[.content.vmiddle.center[

&lt;img src="index_files/figure-html/unnamed-chunk-28-1.png" width="504" /&gt;
]]




---

# Decision Trees

---

# Decision Trees in R using `rpart`

---

class: split-two white

.column[.content[


```r
library(rpart)
library(rpart.plot)

tree &lt;- rpart(Species ~ ., data = iris, method = "class")
rpart.plot(tree)
```

&lt;img src="index_files/figure-html/unnamed-chunk-29-1.png" width="504" /&gt;
]]

.column.bg-main3[.content.vmiddle.center[
## Using the r part function
]]



---

# A single tree is prone to overfitting

&lt;br&gt;

&lt;center&gt;

  &lt;img src="images/overfit.jpg", width="60%"&gt;

&lt;/center&gt;

&lt;br&gt;


---

# Concept: Bagging


---

# Random Forest


---

class: split-two white

.column[.content[


```r
library(randomForest)
set.seed(1)
iris.rf &lt;- randomForest(Species ~ ., iris)
iris.rf
```

```
## 
## Call:
##  randomForest(formula = Species ~ ., data = iris) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 4%
## Confusion matrix:
##            setosa versicolor virginica class.error
## setosa         50          0         0        0.00
## versicolor      0         47         3        0.06
## virginica       0          3        47        0.06
```
]]

.column.bg-main3[.content.vmiddle.center[
## Using the `randomForest`  function
]]

---

class: pink-code

# .purple[A bigger example]

### Let's look at the `SRBCT` dataset, from the package `multiDA`. This data has a response variable `vy`, which is a `factor` of four different cancer subtypes, and gene expression data from 1586 genes. In total, there are 63 observations.


```r
library(multiDA)
class(SRBCT$vy)
```

```
## [1] "factor"
```

```r
table(SRBCT$vy)
```

```
## 
##  1  2  3  4 
##  8 23 12 20
```

```r
dim(SRBCT$mX)
```

```
## [1]   63 1586
```


---

class: split-two white

.column[.content[


```r
*tree.SRBCT &lt;- rpart(SRBCT$vy ~ SRBCT$mX, method = "class")
rpart.plot(tree.SRBCT)
```

&lt;img src="index_files/figure-html/unnamed-chunk-32-1.png" width="504" /&gt;
]]

.column.bg-main3[.content.vmiddle.center[
## Using the r part function
]]

---

class: split-two white

.column[.content[


```r
set.seed(1)
*SRBCT.rf &lt;- randomForest(SRBCT$mX, SRBCT$vy)
SRBCT.rf
```

```
## 
## Call:
##  randomForest(x = SRBCT$mX, y = SRBCT$vy) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 39
## 
##         OOB estimate of  error rate: 0%
## Confusion matrix:
##   1  2  3  4 class.error
## 1 8  0  0  0           0
## 2 0 23  0  0           0
## 3 0  0 12  0           0
## 4 0  0  0 20           0
```
]]

.column.bg-main3[.content.vmiddle.center[
## Using the `randomForest`  function
]]


---


class: split-two white

.column[.content[


```r
set.seed(1)
*SRBCT.rf &lt;- randomForest(SRBCT$mX, as.numeric(SRBCT$vy))
SRBCT.rf
```

```r
Call:
 randomForest(x = SRBCT$mX, y = as.numeric(SRBCT$vy)) 
*              Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 528

          Mean of squared residuals: 0.1641464
                    % Var explained: 85.07
```

]]

.column.bg-main3[.content.vmiddle.center[

<span>&lt;i class="fas  fa-exclamation-triangle fa-2x faa-flash animated "&gt;&lt;/i&gt;</span> Warning: when using `randomForest` make sure that your response is a **factor** if you want classification. If `SRBCT$vy` had been *numeric*, `randomForest` would have instead tried to *regress* the predictors to predict a continuous response!

]]
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
